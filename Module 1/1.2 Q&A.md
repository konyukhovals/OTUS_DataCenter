# Table of contents

[ECMP (на примере OSPF) И UCMP](#item-one)
[Неблокируемая матрица коммутации](#item-two)
[Переподписка в сети CLOS](#item-three)
 

 <!-- headings -->


<a id="item-one"></a>
## ECMP (на примере OSPF) И UCMP 

* ECMP (Equal Cost Multi-Path) в OSPF
>
>OSPF может одновременно использовать несколько маршрутов с одинаковым метриком (Cost) до одной цели.
>Это даёт балансировку нагрузки по нескольким равнозначным путям (например, 2 или 4).
>В Cisco это настраивается параметром maximum-paths внутри router ospf. Пример:
>
>
```
router ospf 1
  network 10.0.0.0 0.255.255.255 area 0
  maximum-paths 4
```

* В таблице маршрутизации при наличии 2 путей с одинаковым Cost OSPF покажет оба next-hop.

```
    R1# show ip route 10.1.1.0
    O 10.1.1.0/24 [110/20] via 192.168.1.2, 00:00:12, GigabitEthernet0/0
                        via 192.168.2.2, 00:00:12, GigabitEthernet0/1
```
Здесь два равных пути (Cost=20).


* UCMP (Unequal Cost Multi-Path)
>
>Позволяет использовать разные пути с разным Cost для балансировки. Чаще встречается в EIGRP (через variance) или в проприетарных расширениях.
>Стандартный OSPF не умеет UCMP «из коробки» – он игнорирует пути с большим Cost, выбирая только равные минимальные.
>Пример в EIGRP (не OSPF), где настроен UCMP:

```
    router eigrp 1
      network 10.0.0.0
      variance 2
      maximum-paths 4
```

Это разрешает привлекать маршруты, чей метрик не превышает минимальный в 2 раза, и балансировать трафик пропорционально.

* Итог

>ECMP в OSPF – равные пути, стандартный функционал, балансировка «1:1».
>UCMP – балансировка с разными весами (не стандарт для OSPF).

* В ospf - по какому признаку балансируется нагрузка в ECMP? 

В OSPF сама балансировка при наличии нескольких равных (Equal Cost) маршрутов заключается в том, что протокол устанавливает в таблицу маршрутизации сразу несколько next-hop с одинаковой метрикой. Как именно трафик распределяется между этими next-hop, обычно зависит не от самого OSPF, а от механизма форвардинга (CEF на Cisco, PF на Juniper и т.п.).
1. По какому признаку идёт балансировка по умолчанию?

* На большинстве платформ (Cisco, Juniper и др.) многопутевая маршрутизация (ECMP) делается по хэшированию комбинации полей пакета (обычно src IP, dst IP, иногда src/dst порт, протокол и т.д.).
* Смысл в том, чтобы один поток (flow) не «прыгал» между разными путями (не ломал TCP-сессии).
* В Cisco IOS классическое поведение – per-destination load balancing (по умолчанию в CEF). То есть исходя из IP-адресов (и, в зависимости от платформы, от других полей) вычисляется хэш, и весь трафик данного потока идёт одним next-hop.

2. Можно ли менять признаки (перенастроить хэш)?

* Да, но это зависит от платформы/OS, а не от OSPF.
* На Cisco IOS можно переключить на per-packet load balancing (тогда каждая следующая пачка пакетов идёт по разным путям). Делается так:

```
router ospf 1
  maximum-paths 4
!
ip cef
!
interface GigabitEthernet0/0
  ip load-sharing per-packet
```
Однако «per-packet» редко используют в продакшене из-за возможных проблем с reorder пакетов.
Для «fine-tune» хэширования (например, учитывать или нет порты) на некоторых платформах Cisco (Catalyst, Nexus) или на Juniper можно изменять так называемый load-balance hashing. В Cisco:

```
(config)# mlsh ?  <- (на некоторых моделях Catalyst)
```
Или в NX-OS:
```
    port-channel load-balance <...>
```
    Для routed-трафика (ECMP) используется схожая логика, где иногда можно добавить/убрать учет L4 портов или MAC и т. д.

3. Итог

* OSPF просто ставит несколько равных маршрутов (ECMP) в RIB, а дальше механизм форвардинга (CEF/другая технология) делает балансировку.
* По умолчанию это «per-destination» (хэш на базе IP + другие поля).









<a id="item-two"></a>
## Неблокируемая матрица коммутации

Неблокируемая матрица коммутации (non-blocking switching fabric) – это свойство коммутатора (или общей коммутационной структуры), при котором все порты могут передавать данные на полную скорость одновременно, без внутреннего «затыка» на уровне бэкуса или внутренних очередей. Проще говоря, если у вас, например, 48 портов 10G, и внутренняя матрица коммутатора «неблокируемая», то суммарный пропускной канал внутри устройства должен быть ≥ 480 Гбит/с (плюс необходимое резервирование на процесс переключения). В таком случае никакие комбинации направлений трафика не приводят к тому, что часть портов будет искусственно «придушена» самим коммутатором.

Что такое «бэкус»?
Чаще всего под «бэкус» (неформальная транслитерация) имеют в виду «бекплейн» (backplane) – внутреннюю высокоскоростную шину/фабрику коммутации в коммутаторе или шасси. Это тот самый аппаратный ресурс (Crossbar, Fabric, Switching ASIC и т.п.), который обеспечивает передачу данных между портами/модулями. Если у коммутатора «неблокируемый бекплейн», значит его внутренняя пропускная способность достаточна, чтобы все порты могли работать на 100% одновременно без внутреннего «узкого места».


<a id="item-three"></a>
## Переподписка в сети CLOS
Переподписка (oversubscription) в сетях с архитектурой CLOS (или «spine-leaf») – это соотношение между «суммарной пропускной способностью доступа (leaf)» и «пропускной способностью магистрали (uplink/spine)»; когда пропускная способность яруса spine меньше, чем суммарный трафик, который потенциально могут сгенерировать leaf-коммутаторы. Например, если у каждого leaf-коммутатора 48 портов 10G «к серверу», а вверх (к spine) у него всего 4 порта 10G, то переподписка будет 48:4 = 12:1.

    Если бы в «идеальной» сети CLOS любой leaf имел достаточное количество uplink-портов (с нужной суммарной полосой пропускания), чтобы суммарная пропускная способность к spine была равна сумме всех портов к серверам, сеть можно было назвать неблокируемой для трафика между любыми leaf. Но на практике это слишком дорого и физически сложно (слишком много кабелей/портов).

    Поэтому связь с понятием «неблокирующая матрица коммутации» в том, что внутри одного коммутатора (или шасси) фабрика может быть «non-blocking» (все порты одновременно на полной скорости). Но в масштабе сети (нескольких коммутаторов в схеме CLOS) нередко делают сознательную переподписку, чтобы сэкономить ресурсы и не строить «идеально» неблокирующий фулл-мэш. В результате при пиковых нагрузках из разных leaf одновременно может возникать «бутылочное горлышко» в spine.

Итого:

    «Неблокируемая матрица» = свойство внутри одного коммутатора (или одной аппаратной фабрики), когда он в состоянии пропускать полный трафик всех портов.
    «Переподписка (oversubscription) в сети CLOS» = свойство между коммутаторами (leaf↔spine), когда суммарная пропускная способность доступа серверов выше, чем суммарная пропускная способность uplink к spine. Это частично «блокирует» (ограничивает) общую пропускную способность между leaf, если все пытаются передавать одновременно.




Рекомендованная переподписка (oversubscription) в CLOS
Нет единой жёстко прописанной «универсальной» цифры. Выбор коэффициента зависит от задач и типов трафика. Однако в большинстве дата-центров для сетей типа leaf-spine применяют практические «типовые» значения:

    1:1 (нет переподписки) – для HPC или очень требовательных сред, где нужна гарантированная максимальная пропускная способность. Дорого и не всегда оправдано.
    3:1, 4:1 – часто встречающийся вариант в коммерческих ЦОДах, где предполагается, что не все одновременно будут «ш saturate» все каналы.
    5:1, 6:1, 7:1 и выше – для сред с более «бурстовым» трафиком, когда постоянная полная загрузка каналов маловероятна.

Таким образом, «рекомендованная» переподписка обычно лежит в диапазоне от 3:1 до 5:1 для большинства корпоративных/провайдерских сетей CLOS. Но конкретная цифра подбирается исходя из профиля трафика, бюджета, количества серверов и SLAs.















* Да, можно настроить «per-packet» или другой вариант хэширования (включить/выключить учёт портов) – но это настройки дата-плана (CEF, порт-ченнелы), а не самого OSPF.



