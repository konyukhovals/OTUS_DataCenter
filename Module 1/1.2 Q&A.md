# Table of contents

[ECMP (на примере OSPF) И UCMP](#item-one)

[Неблокируемая матрица коммутации](#item-two)

[Переподписка в сети CLOS](#item-three)
 
[multihomed multisite multipod superspine borderleaf](#item-four)

 <!-- headings -->


<a id="item-one"></a>
## ECMP (на примере OSPF) И UCMP 

* ECMP (Equal Cost Multi-Path) в OSPF
>
>OSPF может одновременно использовать несколько маршрутов с одинаковым метриком (Cost) до одной цели.
>Это даёт балансировку нагрузки по нескольким равнозначным путям (например, 2 или 4).
>В Cisco это настраивается параметром maximum-paths внутри router ospf. Пример:
>
>
```
router ospf 1
  network 10.0.0.0 0.255.255.255 area 0
  maximum-paths 4
```

* В таблице маршрутизации при наличии 2 путей с одинаковым Cost OSPF покажет оба next-hop.

```
    R1# show ip route 10.1.1.0
    O 10.1.1.0/24 [110/20] via 192.168.1.2, 00:00:12, GigabitEthernet0/0
                        via 192.168.2.2, 00:00:12, GigabitEthernet0/1
```
Здесь два равных пути (Cost=20).


* UCMP (Unequal Cost Multi-Path)
>
>Позволяет использовать разные пути с разным Cost для балансировки. Чаще встречается в EIGRP (через variance) или в проприетарных расширениях.
>Стандартный OSPF не умеет UCMP «из коробки» – он игнорирует пути с большим Cost, выбирая только равные минимальные.
>Пример в EIGRP (не OSPF), где настроен UCMP:

```
    router eigrp 1
      network 10.0.0.0
      variance 2
      maximum-paths 4
```

Это разрешает привлекать маршруты, чей метрик не превышает минимальный в 2 раза, и балансировать трафик пропорционально.

* Итог

>ECMP в OSPF – равные пути, стандартный функционал, балансировка «1:1».
>UCMP – балансировка с разными весами (не стандарт для OSPF).

* В ospf - по какому признаку балансируется нагрузка в ECMP? 

В OSPF сама балансировка при наличии нескольких равных (Equal Cost) маршрутов заключается в том, что протокол устанавливает в таблицу маршрутизации сразу несколько next-hop с одинаковой метрикой. Как именно трафик распределяется между этими next-hop, обычно зависит не от самого OSPF, а от механизма форвардинга (CEF на Cisco, PF на Juniper и т.п.).
1. По какому признаку идёт балансировка по умолчанию?

* На большинстве платформ (Cisco, Juniper и др.) многопутевая маршрутизация (ECMP) делается по хэшированию комбинации полей пакета (обычно src IP, dst IP, иногда src/dst порт, протокол и т.д.).
* Смысл в том, чтобы один поток (flow) не «прыгал» между разными путями (не ломал TCP-сессии).
* В Cisco IOS классическое поведение – per-destination load balancing (по умолчанию в CEF). То есть исходя из IP-адресов (и, в зависимости от платформы, от других полей) вычисляется хэш, и весь трафик данного потока идёт одним next-hop.

2. Можно ли менять признаки (перенастроить хэш)?

* Да, но это зависит от платформы/OS, а не от OSPF.
* На Cisco IOS можно переключить на per-packet load balancing (тогда каждая следующая пачка пакетов идёт по разным путям). Делается так:

```
router ospf 1
  maximum-paths 4
!
ip cef
!
interface GigabitEthernet0/0
  ip load-sharing per-packet
```
Однако «per-packet» редко используют в продакшене из-за возможных проблем с reorder пакетов.
Для «fine-tune» хэширования (например, учитывать или нет порты) на некоторых платформах Cisco (Catalyst, Nexus) или на Juniper можно изменять так называемый load-balance hashing. В Cisco:

```
(config)# mlsh ?  <- (на некоторых моделях Catalyst)
```
Или в NX-OS:
```
    port-channel load-balance <...>
```
    Для routed-трафика (ECMP) используется схожая логика, где иногда можно добавить/убрать учет L4 портов или MAC и т. д.

3. Итог

* OSPF просто ставит несколько равных маршрутов (ECMP) в RIB, а дальше механизм форвардинга (CEF/другая технология) делает балансировку.
* По умолчанию это «per-destination» (хэш на базе IP + другие поля).
* Да, можно настроить «per-packet» или другой вариант хэширования (включить/выключить учёт портов) – но это настройки дата-плана (CEF, порт-ченнелы), а не самого OSPF.


<a id="item-two"></a>
## Неблокируемая матрица коммутации

Неблокируемая матрица коммутации (non-blocking switching fabric) – это свойство коммутатора (или общей коммутационной структуры), при котором все порты могут передавать данные на полную скорость одновременно, без внутреннего «затыка» на уровне бэкуса или внутренних очередей. Проще говоря, если у вас, например, 48 портов 10G, и внутренняя матрица коммутатора «неблокируемая», то суммарный пропускной канал внутри устройства должен быть ≥ 480 Гбит/с (плюс необходимое резервирование на процесс переключения). В таком случае никакие комбинации направлений трафика не приводят к тому, что часть портов будет искусственно «придушена» самим коммутатором.

Что такое «бэкус»?
Чаще всего под «бэкус» (неформальная транслитерация) имеют в виду «бекплейн» (backplane) – внутреннюю высокоскоростную шину/фабрику коммутации в коммутаторе или шасси. Это тот самый аппаратный ресурс (Crossbar, Fabric, Switching ASIC и т.п.), который обеспечивает передачу данных между портами/модулями. Если у коммутатора «неблокируемый бекплейн», значит его внутренняя пропускная способность достаточна, чтобы все порты могли работать на 100% одновременно без внутреннего «узкого места».

«Неблокируемая матрица» = свойство внутри одного коммутатора (или одной аппаратной фабрики), когда он в состоянии пропускать полный трафик всех портов.

<a id="item-three"></a>
## Переподписка в сети CLOS
Переподписка (oversubscription) в сетях с архитектурой CLOS (или «spine-leaf») – это соотношение между «суммарной пропускной способностью доступа (leaf)» и «пропускной способностью магистрали (uplink/spine)»; когда пропускная способность яруса spine меньше, чем суммарный трафик, который потенциально могут сгенерировать leaf-коммутаторы. Например, если у каждого leaf-коммутатора 48 портов 10G «к серверу», а вверх (к spine) у него всего 4 порта 10G, то переподписка будет 48:4 = 12:1.

* Если бы в «идеальной» сети CLOS любой leaf имел достаточное количество uplink-портов (с нужной суммарной полосой пропускания), чтобы суммарная пропускная способность к spine была равна сумме всех портов к серверам, сеть можно было назвать неблокируемой для трафика между любыми leaf. Но на практике это слишком дорого и физически сложно (слишком много кабелей/портов).

* Поэтому связь с понятием «неблокирующая матрица коммутации» в том, что внутри одного коммутатора (или шасси) фабрика может быть «non-blocking» (все порты одновременно на полной скорости). Но в масштабе сети (нескольких коммутаторов в схеме CLOS) нередко делают сознательную переподписку, чтобы сэкономить ресурсы и не строить «идеально» неблокирующий фулл-мэш. В результате при пиковых нагрузках из разных leaf одновременно может возникать «бутылочное горлышко» в spine.
* «Переподписка (oversubscription) в сети CLOS» = свойство между коммутаторами (leaf↔spine), когда суммарная пропускная способность доступа серверов выше, чем суммарная пропускная способность uplink к spine. Это частично «блокирует» (ограничивает) общую пропускную способность между leaf, если все пытаются передавать одновременно.

* Рекомендованная переподписка (oversubscription) в CLOS
Нет единой жёстко прописанной «универсальной» цифры. Выбор коэффициента зависит от задач и типов трафика. Однако в большинстве дата-центров для сетей типа leaf-spine применяют практические «типовые» значения:

* * 1:1 (нет переподписки) – для HPC или очень требовательных сред, где нужна гарантированная максимальная пропускная способность. Дорого и не всегда оправдано.
* * 3:1, 4:1 – часто встречающийся вариант в коммерческих ЦОДах, где предполагается, что не все одновременно будут «ш saturate» все каналы.
* * 5:1, 6:1, 7:1 и выше – для сред с более «бурстовым» трафиком, когда постоянная полная загрузка каналов маловероятна.

Таким образом, «рекомендованная» переподписка обычно лежит в диапазоне от 3:1 до 5:1 для большинства корпоративных/провайдерских сетей CLOS. Но конкретная цифра подбирается исходя из профиля трафика, бюджета, количества серверов и SLAs.

<a id="item-four"></a>
## multihomed multisite multipod superspine borderleaf

#### Multihomed

Определение

    «Multihomed» в контексте CLOS означает, что узел (сервер или TOR-коммутатор leaf) подключён к нескольким вышестоящим устройствам (spine или другим leaf), обеспечивая резервирование и балансировку.
    Часто речь идёт о сервере с несколькими сетевыми картами (NIC), включёнными в разные leaf, или о leaf-коммутаторе, который имеет несколько uplink-подключений к spine-коммутаторам.

Пример конфигурации (BGP) на leaf, у которого 2 аплинка к spine
```
! Leaf-коммутатор (система AS 65001)
feature bgp

router bgp 65001
  router-id 10.0.0.1
  neighbor 10.1.1.1 remote-as 65000     ! spine1
  neighbor 10.1.2.1 remote-as 65000     ! spine2
  address-family ipv4 unicast
    network 192.168.1.0/24
```
Здесь leaf «multihomed» – он формирует BGP-пиры сразу с двумя spine. При выходе из строя одного соединения оставшееся продолжит обслуживать трафик.

#### Multisite

Определение

    «Multisite» означает, что у организации есть несколько географически разнесённых дата-центров (каждый со своей CLOS-фабрикой).
    Между этими сайтами нередко строят IP WAN, VXLAN/EVPN-туннели или VPN, чтобы связать их в единую сеть (L3 DCI, иногда L2 extension при необходимости).

Примерная схема
```
[Site1: Leaf-Spine Fabric] ---[WAN / IP Core]--- [Site2: Leaf-Spine Fabric]
```
Каждый сайт имеет собственные spine и leaf. Для «multisite» часто используют EVPN (BGP), а поверх — IP/MPLS или VXLAN с маршрутированием между AS (eBGP).


#### Multipod

Определение

    «Multipod» – это когда внутри одного дата-центра CLOS-фабрика разделена на несколько «подов» (Pod1, Pod2, …). Каждый под – это отдельный набор leaf-коммутаторов, подключённый к общим spine или к так называемому супер spine (см. следующий пункт).
    Поды бывают нужны для разделения на логические зоны, разные группы сервисов, упрощения масштабирования.

```
Примерная схема

        [ Super-Spine ]
          /      \
   [ Spine-Pod1 ] [ Spine-Pod2 ]
      /   \          /   \
   [Leaf] [Leaf]  [Leaf] [Leaf]
```

    Каждый Pod имеет собственную «вершину» spine-подъем, а все вместе сводятся либо в общий слой «Super-Spine», либо объединяются напрямую (см. topologies ACI Multipod, Arista Multi-Pod и др.).

#### Superspine

    «Superspine» (или «Super Spine») – это ещё один иерархический уровень в больших CLOS-сетях, который находится выше обычных spine.
    Применяется для связки нескольких больших «pod» или для очень крупных фабрик, где одного слоя spine недостаточно.
    Super-Spine-коммутаторы должны иметь максимально быструю неблокирующую фабрику, часто это самые высокопроизводительные устройства в сети.

Пример
В Cisco ACI Multi-Pod Super-Spine – это отдельный (малочисленный) уровень коммутаторов, которые соединяют Pods между собой, сохраняя слой spine-под внутри каждого Pod.


#### Border Leaf

    «Border Leaf» – это leaf-коммутатор (иногда выделенная роль), который отвечает за подключение к внешним сетям (WAN, Internet, MPLS, корпоративной L3-сети).
    В VXLAN/EVPN-фабриках на Border Leaf обычно размещают Anycast Gateway для внешней маршрутизации, или делают Route-Reflector, или подключают firewalls.

Пример конфигурации (поддержка внешнего BGP)

```
router bgp 65001
  address-family ipv4 unicast
    neighbor 203.0.113.1 remote-as 65010     ! Провайдер/WAN
    neighbor 203.0.113.1 next-hop-self
    redistribute connected
!
interface Ethernet1/1
  description "Uplink to WAN Router"
  ip address 203.0.113.2/30
```

Этот Leaf выполняет роль пограничного: в CLOS он связан с другими spine (внутренняя фабрика), а наружу – к провайдеру или междоменному маршрутизатору.
Итоговая сводка

* Multihomed: узел или leaf с несколькими физическими аплинками для резервирования.
* Multisite: несколько географически разнесённых CLOS-фабрик, объединённых в общую сеть.
* Multipod: разделение одной крупной фабрики на несколько «под-» сегментов (Pod1, Pod2...), часто с общим или суперспайн-уровнем.
* Superspine: дополнительный слой выше spine для связки множества pod или сверхкрупных фабрик.
* Border Leaf: leaf-коммутатор, обслуживающий «выход наружу» (WAN/Internet/MPLS).












